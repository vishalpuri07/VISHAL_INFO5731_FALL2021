{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USSdXHuqnwv9"
   },
   "source": [
    "# **INFO5731 Assignment Four**\n",
    "\n",
    "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWxodXh5n4xF"
   },
   "source": [
    "# **Question 1: Topic Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TenBkDJ5n95k"
   },
   "source": [
    "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA and LSA. The following information should be reported:\n",
    "\n",
    "(1) Features (top n-gram phrases) used for topic modeling.\n",
    "\n",
    "(2) Top 10 clusters for topic modeling.\n",
    "\n",
    "(3) Summarize and describe the topic for each cluster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "PuFPKhC0m1fd",
    "outputId": "bc3a25b7-800e-459a-fc77-17ab1be4c7e8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # Import pandas as pd and import the lemmatized data used in previous assignments.\n",
    "df = pd.read_csv(\"data_lemmatized.csv\") # Contains 500 tweets for search word \"Data Mining\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FxdEL4c1GF35"
   },
   "outputs": [],
   "source": [
    "import numpy as np # Using the code from assignment 03 to clean the lemmatized data so that it can be used as list.\n",
    "from numpy import nan\n",
    "\n",
    "df = pd.read_csv(\"data_lemmatized.csv\") # Contains 500 tweets for search word \"Data Mining\"\n",
    "\n",
    "# Since the list new contains nan values now that it is extracted from csv. remove all of those.\n",
    "new_list = df.values.tolist()\n",
    "for i in range(0,500):\n",
    "  del new_list[i][0] # removes the index\n",
    "  new_list[i] = list(set(new_list[i])) # removes all the nan and bring it to 1\n",
    "  if new_list[i].count(nan) == 1: # remove all nan values\n",
    "    new_list[i].remove(nan)\n",
    "  else:\n",
    "    1 ==1\n",
    "    \n",
    "data_lemmatized = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XK_Q86nHGdxH"
   },
   "outputs": [],
   "source": [
    "# Import all the necessary libraries for the assignment\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "\n",
    "# Create dictionary using corpora module and create the corpus from the lemmatized data\n",
    "id2word = corpora.Dictionary(data_lemmatized) # from the link above\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZMG76K3tGfEl"
   },
   "outputs": [],
   "source": [
    "# Build base LDA Model\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,id2word=id2word, num_topics=20, random_state=100,chunksize=100,passes=10,per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top n-grams used for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j4gi8JxbGxAT",
    "outputId": "cb5908ef-a3fc-4045-ba4d-f11348d656b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.073*\"datum\" + 0.061*\"miner\" + 0.056*\"ga\" + 0.056*\"unload\" + '\n",
      "  '0.056*\"center_operation\" + 0.056*\"college_park\" + 0.056*\"new\" + '\n",
      "  '0.017*\"hour\" + 0.017*\"mine\" + 0.017*\"tagcoin\"'),\n",
      " (1,\n",
      "  '0.066*\"mining\" + 0.066*\"datum\" + 0.061*\"people\" + 0.055*\"taking_advantage\" '\n",
      "  '+ 0.055*\"call\" + 0.055*\"canada_action\" + 0.018*\"internet\" + 0.016*\"privacy\" '\n",
      "  '+ 0.016*\"privacymatter\" + 0.016*\"protection\"'),\n",
      " (2,\n",
      "  '0.038*\"facebook\" + 0.038*\"tha\" + 0.038*\"ongoing\" + 0.038*\"war\" + '\n",
      "  '0.038*\"completely\" + 0.038*\"current\" + 0.038*\"ignore\" + 0.038*\"shitstorm\" + '\n",
      "  '0.038*\"acczibit\" + 0.010*\"schedule\"'),\n",
      " (3,\n",
      "  '0.056*\"datum\" + 0.047*\"power\" + 0.042*\"tagcoin\" + 0.042*\"one\" + 0.042*\"see\" '\n",
      "  '+ 0.037*\"real\" + 0.032*\"mine\" + 0.027*\"co\" + 0.026*\"backed_nft\" + '\n",
      "  '0.016*\"help\"'),\n",
      " (4,\n",
      "  '0.080*\"company\" + 0.080*\"try\" + 0.077*\"family\" + 0.077*\"got_mad\" + '\n",
      "  '0.077*\"garland\" + 0.077*\"million\" + 0.077*\"question\" + 0.077*\"panorama\" + '\n",
      "  '0.077*\"made_via\" + 0.077*\"education\"'),\n",
      " (5,\n",
      "  '0.034*\"co\" + 0.034*\"mining\" + 0.028*\"datum\" + 0.019*\"social_media\" + '\n",
      "  '0.018*\"title\" + 0.018*\"rule\" + 0.018*\"thesis\" + 0.018*\"prediction\" + '\n",
      "  '0.018*\"robust\" + 0.014*\"get\"'),\n",
      " (6,\n",
      "  '0.050*\"co\" + 0.042*\"https\" + 0.035*\"datum\" + 0.022*\"mining\" + 0.013*\"drone\" '\n",
      "  '+ 0.012*\"work\" + 0.012*\"use\" + 0.012*\"analytic\" + 0.009*\"computer\" + '\n",
      "  '0.009*\"engineering\"'),\n",
      " (7,\n",
      "  '0.103*\"mining\" + 0.100*\"child\" + 0.100*\"data\" + 0.100*\"school\" + '\n",
      "  '0.100*\"company\" + 0.098*\"call\" + 0.098*\"run\" + 0.098*\"involve\" + '\n",
      "  '0.098*\"family\" + 0.002*\"conduct\"'),\n",
      " (8,\n",
      "  '0.107*\"mining\" + 0.087*\"data\" + 0.083*\"co\" + 0.066*\"research\" + 0.065*\"web\" '\n",
      "  '+ 0.063*\"entry_scrape\" + 0.063*\"business_datascraper\" + 0.027*\"https\" + '\n",
      "  '0.020*\"datum\" + 0.018*\"excel_google\"'),\n",
      " (9,\n",
      "  '0.048*\"co\" + 0.017*\"https\" + 0.010*\"garland\" + 0.009*\"datum\" + '\n",
      "  '0.009*\"internet\" + 0.008*\"base\" + 0.008*\"angrybklynmom\" + 0.008*\"patte\" + '\n",
      "  '0.008*\"still\" + 0.008*\"lot\"'),\n",
      " (10,\n",
      "  '0.083*\"co\" + 0.065*\"https\" + 0.063*\"favourite_serie\" + '\n",
      "  '0.063*\"netflix_wherever\" + 0.059*\"know\" + 0.059*\"free_download\" + '\n",
      "  '0.056*\"enjoy\" + 0.053*\"daisy_thee\" + 0.021*\"mining\" + 0.016*\"datum\"'),\n",
      " (11,\n",
      "  '0.040*\"mining\" + 0.037*\"hard_ni\" + 0.037*\"like_pac\" + 0.037*\"pac\" + '\n",
      "  '0.037*\"heavily_influence\" + 0.037*\"hate\" + 0.037*\"dunno\" + 0.033*\"outcome\" '\n",
      "  '+ 0.033*\"election\" + 0.021*\"truth\"'),\n",
      " (12,\n",
      "  '0.066*\"mining\" + 0.046*\"co\" + 0.034*\"datum\" + 0.017*\"https\" + 0.014*\"get\" + '\n",
      "  '0.013*\"data\" + 0.012*\"game\" + 0.012*\"sympathetic\" + 0.012*\"human\" + '\n",
      "  '0.012*\"new\"'),\n",
      " (13,\n",
      "  '0.051*\"mining\" + 0.035*\"co\" + 0.032*\"datum\" + 0.020*\"https\" + 0.015*\"care\" '\n",
      "  '+ 0.015*\"use\" + 0.010*\"data\" + 0.010*\"band\" + 0.010*\"set\" + '\n",
      "  '0.010*\"primary\"'),\n",
      " (14,\n",
      "  '0.050*\"co\" + 0.040*\"mining\" + 0.031*\"datum\" + 0.028*\"bitcoin\" + '\n",
      "  '0.028*\"https\" + 0.016*\"global\" + 0.012*\"get\" + 0.011*\"customer\" + '\n",
      "  '0.011*\"base\" + 0.011*\"eth\"'),\n",
      " (15,\n",
      "  '0.081*\"mining\" + 0.037*\"co\" + 0.029*\"project\" + 0.028*\"data\" + '\n",
      "  '0.017*\"mayor\" + 0.017*\"water\" + 0.017*\"infrastructure\" + 0.017*\"require\" + '\n",
      "  '0.016*\"datum\" + 0.009*\"company\"'),\n",
      " (16,\n",
      "  '0.027*\"receipt\" + 0.027*\"pm\" + 0.027*\"tune\" + 0.027*\"take\" + 0.027*\"get\" + '\n",
      "  '0.027*\"tuckercarlson\" + 0.027*\"show\" + 0.027*\"tonight\" + 0.027*\"et\" + '\n",
      "  '0.020*\"late\"'),\n",
      " (17,\n",
      "  '0.051*\"co\" + 0.036*\"mining\" + 0.028*\"https\" + 0.017*\"datum\" + 0.013*\"data\" '\n",
      "  '+ 0.012*\"tagprotocol\" + 0.011*\"oi\" + 0.007*\"run\" + 0.006*\"check\" + '\n",
      "  '0.006*\"get\"'),\n",
      " (18,\n",
      "  '0.056*\"mining\" + 0.051*\"co\" + 0.020*\"data\" + 0.019*\"bitcoin\" + '\n",
      "  '0.015*\"https\" + 0.014*\"power\" + 0.014*\"supply\" + 0.014*\"energy\" + '\n",
      "  '0.012*\"get\" + 0.011*\"datum\"'),\n",
      " (19,\n",
      "  '0.060*\"mining\" + 0.055*\"datum\" + 0.054*\"co\" + 0.025*\"https\" + 0.017*\"say\" + '\n",
      "  '0.014*\"need\" + 0.013*\"elcostello\" + 0.013*\"nwhich\" + 0.013*\"even\" + '\n",
      "  '0.013*\"true\"')]\n"
     ]
    }
   ],
   "source": [
    "# View Topics base on the base model\n",
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8le2f99Gzhp",
    "outputId": "bb3ed6e6-12ce-46a2-9cf6-76069453dba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.33938459434500334\n"
     ]
    }
   ],
   "source": [
    "# Base Line Coherence - This is the coherence \n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i7a4ck90G2UB"
   },
   "outputs": [],
   "source": [
    "# OPTIMUM NUMBER OF K-TOPICS BASED ON COHORENCE SCORE Function.\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shows different values of coherence for increase in num of topics by 2. We need to choose 10 topics for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "YjizfHn9G5Gf",
    "outputId": "1cf799a2-e490-4d0e-ea99-9f58c4ef98d0"
   },
   "outputs": [],
   "source": [
    "import os # This for gensim model to work that provides \n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "os.environ.update({'MALLET_HOME':r'C:/new_mallet/mallet-2.0.8/'})\n",
    "mallet_path = r'C:/new_mallet/mallet-2.0.8/bin/mallet'\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=12, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plots\n",
    "# Show graph\n",
    "limit=12; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "plots.plot(x, coherence_values)\n",
    "plots.xlabel(\"Num Topics\")\n",
    "plots.ylabel(\"Coherence score\")\n",
    "plots.legend((\"coherence_values\"), loc='best')\n",
    "plots.ylim([0.45, 0.85])\n",
    "plots.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Topics = 2  has Coherence Value of 0.4558\n",
      "Num Topics = 4  has Coherence Value of 0.4715\n",
      "Num Topics = 6  has Coherence Value of 0.558\n",
      "Num Topics = 8  has Coherence Value of 0.4614\n",
      "Num Topics = 10  has Coherence Value of 0.5412\n"
     ]
    }
   ],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Choose top 10 clusters for topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.186*\"call\" + 0.173*\"family\" + 0.139*\"involve\" + 0.124*\"school\" + '\n",
      "  '0.110*\"run\" + 0.098*\"child\" + 0.041*\"company\" + 0.004*\"pro\" + 0.004*\"tune\" '\n",
      "  '+ 0.004*\"peak\"'),\n",
      " (1,\n",
      "  '0.062*\"bitcoin\" + 0.043*\"miner\" + 0.031*\"unload\" + 0.031*\"college_park\" + '\n",
      "  '0.031*\"center_operation\" + 0.031*\"ga\" + 0.024*\"big\" + 0.024*\"thing\" + '\n",
      "  '0.024*\"base\" + 0.017*\"internet\"'),\n",
      " (2,\n",
      "  '0.256*\"mining\" + 0.253*\"data\" + 0.030*\"provide\" + 0.028*\"webscrape\" + '\n",
      "  '0.025*\"copy_paste\" + 0.025*\"excel_google\" + 0.023*\"service\" + 0.010*\"make\" '\n",
      "  '+ 0.008*\"group\" + 0.008*\"work\"'),\n",
      " (3,\n",
      "  '0.190*\"mining\" + 0.031*\"child\" + 0.028*\"project\" + 0.023*\"like_pac\" + '\n",
      "  '0.023*\"dunno\" + 0.023*\"hard_ni\" + 0.023*\"hate\" + 0.023*\"heavily_influence\" '\n",
      "  '+ 0.023*\"pac\" + 0.021*\"election\"'),\n",
      " (4,\n",
      "  '0.150*\"company\" + 0.148*\"run\" + 0.144*\"school\" + 0.139*\"child\" + '\n",
      "  '0.121*\"involve\" + 0.114*\"call\" + 0.017*\"mining\" + 0.008*\"description\" + '\n",
      "  '0.004*\"repeat\" + 0.004*\"facility\"'),\n",
      " (5,\n",
      "  '0.178*\"family\" + 0.168*\"company\" + 0.062*\"education\" + 0.062*\"garland\" + '\n",
      "  '0.059*\"million\" + 0.059*\"got_mad\" + 0.059*\"made_via\" + 0.059*\"panorama\" + '\n",
      "  '0.059*\"question\" + 0.012*\"child\"'),\n",
      " (6,\n",
      "  '0.389*\"mining\" + 0.065*\"people\" + 0.046*\"taking_advantage\" + '\n",
      "  '0.041*\"canada_action\" + 0.016*\"year\" + 0.014*\"eldenre\" + 0.014*\"ad\" + '\n",
      "  '0.011*\"srlbukdfxa\" + 0.011*\"full\" + 0.011*\"price\"'),\n",
      " (7,\n",
      "  '0.203*\"https\" + 0.050*\"netflix_wherever\" + 0.050*\"favourite_serie\" + '\n",
      "  '0.048*\"free_download\" + 0.045*\"enjoy\" + 0.043*\"daisy_thee\" + 0.018*\"rig\" + '\n",
      "  '0.018*\"social_media\" + 0.015*\"focus\" + 0.015*\"facebook\"'),\n",
      " (8,\n",
      "  '0.289*\"datum\" + 0.039*\"power\" + 0.037*\"mine\" + 0.030*\"gold\" + 0.028*\"learn\" '\n",
      "  '+ 0.028*\"tagcoin\" + 0.019*\"real\" + 0.016*\"machine\" + 0.014*\"ai\" + '\n",
      "  '0.012*\"nft\"'),\n",
      " (9,\n",
      "  '0.259*\"data\" + 0.102*\"research\" + 0.099*\"web\" + '\n",
      "  '0.091*\"business_datascraper\" + 0.091*\"entry_scrape\" + 0.013*\"supply\" + '\n",
      "  '0.010*\"thesis\" + 0.010*\"rule\" + 0.010*\"prediction\" + 0.010*\"robust\"')]\n"
     ]
    }
   ],
   "source": [
    "# Select the top 10 clusters\n",
    "optimal_model = model_list[4]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**<br>\n",
    "(0,'0.186*\"call\" + 0.173*\"family\" + 0.139*\"involve\" + 0.124*\"school\" + '\n",
    "  '0.110*\"run\" + 0.098*\"child\" + 0.041*\"company\" + 0.004*\"pro\" + 0.004*\"tune\" '\n",
    "  '+ 0.004*\"peak\"')<br>\n",
    "This cluster has highest density of word call, family, involve, school, company, tune, peak, run, child, etc. - Based on this it looks like the topic for this cluster is - **Work from Home**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2** <br>(1,\n",
    "  '0.062*\"bitcoin\" + 0.043*\"miner\" + 0.031*\"unload\" + 0.031*\"college_park\" + '\n",
    "  '0.031*\"center_operation\" + 0.031*\"ga\" + 0.024*\"big\" + 0.024*\"thing\" + '\n",
    "  '0.024*\"base\" + 0.017*\"internet\"')<br>\n",
    "This cluster has highest density of words bitcoin, miner, internet, big, thing, college_park etc. - Based on this it looks like the topic for this cluster is - **Bitcoin mining**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3** <br>(2,\n",
    "  '0.256*\"mining\" + 0.253*\"data\" + 0.030*\"provide\" + 0.028*\"webscrape\" + ''0.025*\"copy_paste\" + 0.025*\"excel_google\" + 0.023*\"service\" + 0.010*\"make\" '\n",
    "  '+ 0.008*\"group\" + 0.008*\"work\"')<br>\n",
    "This cluster has highest density of wordsdata, mining, webscrape, copy_paste, service, make etc. - Based on this it looks like the topic for this cluster is - **Web Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4** <br>(3,'0.190*\"mining\" + 0.031*\"child\" + 0.028*\"project\" + 0.023*\"like_pac\" + '\n",
    "  '0.023*\"dunno\" + 0.023*\"hard_ni\" + 0.023*\"hate\" + 0.023*\"heavily_influence\" '\n",
    "  '+ 0.023*\"pac\" + 0.021*\"election\"')<br>\n",
    "This cluster has highest density of child, pac, election, heavy_influence etc. - Based on this it looks like the topic for this cluster is - **Data mining for election**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5** <br>(4,\n",
    "  '0.150*\"company\" + 0.148*\"run\" + 0.144*\"school\" + 0.139*\"child\" + '\n",
    "  '0.121*\"involve\" + 0.114*\"call\" + 0.017*\"mining\" + 0.008*\"description\" + '\n",
    "  '0.004*\"repeat\" + 0.004*\"facility\"')<br>\n",
    "This cluster has highest density of company, run, school, child, involve, call, mining, description, facility etc. - Based on this it looks like the topic for this cluster is - **Mining tutorials for kids**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6** <br>(5,\n",
    "  '0.178*\"family\" + 0.168*\"company\" + 0.062*\"education\" + 0.062*\"garland\" + '\n",
    "  '0.059*\"million\" + 0.059*\"got_mad\" + 0.059*\"made_via\" + 0.059*\"panorama\" + '\n",
    "  '0.059*\"question\" + 0.012*\"child\"')<br>\n",
    "This cluster has highest density of family, education, got_mad, million, question, child, panaroma, garland etc. - Based on this it looks like the topic for this cluster is - **Child's perspective**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7** <br>(6,\n",
    "  '0.389*\"mining\" + 0.065*\"people\" + 0.046*\"taking_advantage\" + '\n",
    "  '0.041*\"canada_action\" + 0.016*\"year\" + 0.014*\"eldenre\" + 0.014*\"ad\" + '\n",
    "  '0.011*\"srlbukdfxa\" + 0.011*\"full\" + 0.011*\"price\"')<br>\n",
    "This cluster has highest density of mining, taking_advantage, canada_action, price, full etc. - Based on this it looks like the topic for this cluster is - **Sales in mining industry**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8** <br>(7,\n",
    "  '0.203*\"https\" + 0.050*\"netflix_wherever\" + 0.050*\"favourite_serie\" + '\n",
    "  '0.048*\"free_download\" + 0.045*\"enjoy\" + 0.043*\"daisy_thee\" + 0.018*\"rig\" + '\n",
    "  '0.018*\"social_media\" + 0.015*\"focus\" + 0.015*\"facebook\"')<br>\n",
    "This cluster has highest density of netflix, social_media, facebook, enjoy etc. - Based on this it looks like the topic for this cluster is - **Social Media**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9** <br>(8,\n",
    "  '0.289*\"datum\" + 0.039*\"power\" + 0.037*\"mine\" + 0.030*\"gold\" + 0.028*\"learn\" '\n",
    "  '+ 0.028*\"tagcoin\" + 0.019*\"real\" + 0.016*\"machine\" + 0.014*\"ai\" + '\n",
    "  '0.012*\"nft\"')<br>\n",
    "This cluster has highest density of datum, power, mine, gold, learn, ai, machine etc. - Based on this it looks like the topic for this cluster is - **Power of data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10** <br>(9,\n",
    "  '0.259*\"data\" + 0.102*\"research\" + 0.099*\"web\" + '\n",
    "  '0.091*\"business_datascraper\" + 0.091*\"entry_scrape\" + 0.013*\"supply\" + '\n",
    "  '0.010*\"thesis\" + 0.010*\"rule\" + 0.010*\"prediction\" + 0.010*\"robust\"')<br>\n",
    "This cluster has highest density of data, web, research, robust, prediction, rule etc. - Based on this it looks like the topic for this cluster is - **prediction using data research**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfpMRCrRwN6Z"
   },
   "source": [
    "# **Question 2: Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dCQEbDawWCw"
   },
   "source": [
    "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
    "\n",
    "(1) Features used for sentiment classification and explain why you select these features.\n",
    "\n",
    "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. \n",
    "\n",
    "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "vATjQNTY8buA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>1</td>\n",
       "      <td>@nbensalem i'm sitting at my house and i'm soo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>1</td>\n",
       "      <td>Ordered some maternity clothes online, which c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>1</td>\n",
       "      <td>@citizensheep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>1</td>\n",
       "      <td>@thesage1014 I wish I could! Gotta work though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>1</td>\n",
       "      <td>@SeaGhostdesigns what happened to you on Satur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1014 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all....\n",
       "...      ...                                                ...\n",
       "1009       1  @nbensalem i'm sitting at my house and i'm soo...\n",
       "1010       1  Ordered some maternity clothes online, which c...\n",
       "1011       1                                     @citizensheep \n",
       "1012       1    @thesage1014 I wish I could! Gotta work though \n",
       "1013       1  @SeaGhostdesigns what happened to you on Satur...\n",
       "\n",
       "[1014 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import tweets data - used different data with more evenly distributed target variable\n",
    "df_tweets = pd.read_csv(\"tweet_sentiment.csv\")\n",
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "\n",
    "#################### TRAINING DATA ###########################\n",
    "# Convert all the words to lower. Example: \"Upper\" and \"upper\" are different because they have different ascii keys.\n",
    "df_tweets['text'] = df_tweets['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# Remove the punctuations and special characters\n",
    "df_tweets['text'] = df_tweets['text'].str.replace(\"@,#,$,!,^,&,*,_,+,=,~,`,,,.,:,;,/,?\",'') # remove puncuation special characters and numbers\n",
    "\n",
    "# remove all the stopwords from english language\n",
    "df_tweets['text'] = df_tweets['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
    "\n",
    "# Tokenize and Lemmatize the texts.\n",
    "df_tweets['text'] = df_tweets['text'].apply(lambda x: TextBlob(x).words)\n",
    "df_tweets['text'] = df_tweets['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to transform the data and then we can split the data to train and test data.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vec = TfidfVectorizer(ngram_range=(1,2), max_features=1000)\n",
    "tf_idf_vec.fit(df_tweets['text'])\n",
    "x_values =  tf_idf_vec.transform(df_tweets['text'])\n",
    "y_values = df_tweets['target']\n",
    "\n",
    "\n",
    "# Split the training data to training and validating data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_values, y_values, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions and variables that will perform the metrics i.e Evaluation measurement:\n",
    "# (1) Accuracy, (2) Recall, (3) Precison, (4) F-1 score \n",
    "# this metrics will be used for all models and we need to write the code again but simply call these functions.\n",
    "\n",
    "# Import metrics module from sklearn.\n",
    "from sklearn import metrics\n",
    "\n",
    "def evaluation(y_pred, y_test):\n",
    "    Accuracy = metrics.accuracy_score(y_pred, y_test)\n",
    "    Recall = metrics.recall_score(y_pred = y_pred, y_true = y_test, pos_label='positive', average='micro') # micro calculates total true positives, false negatives and false positives\n",
    "    Precision = metrics.precision_score(y_pred = y_pred, y_true = y_test, pos_label='positive', average='micro') # micro calculates total true positives, false negatives and false positives\n",
    "    F1 = 2 * (Precision * Recall) / (Precision + Recall) # Formula for F1 Score\n",
    "    print(\"Accuracy: \", Accuracy.round(4))\n",
    "    print(\"Recall:\", Recall.round(4))\n",
    "    print(\"Precision:\", Precision.round(4))\n",
    "    print(\"F-1 score:\", F1.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select two models using scikit learn and compare the provided metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5172\n",
      "Recall: 0.5172\n",
      "Precision: 0.5172\n",
      "F-1 score: 0.5172\n",
      "Cross Validation Score: 0.5576\n"
     ]
    }
   ],
   "source": [
    "from sklearn import naive_bayes\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Create the model.\n",
    "naive_bayes_model = naive_bayes.MultinomialNB()\n",
    "\n",
    "# Fit the training data\n",
    "naive_bayes_model.fit(x_train,y_train) # Fit the model using the training dataset\n",
    "\n",
    "# Pass the validation dataset and see the evaluation scores\n",
    "y_pred_valid = naive_bayes_model.predict(x_valid) # predict y_valid using x_valid\n",
    "evaluation(y_pred_valid, y_valid) # Run the evaluation function that gives evaluation scores\n",
    "print(\"Cross Validation Score:\", cross_val_score(naive_bayes_model, x_valid, y_valid, cv= KFold(10, shuffle=True, random_state = 22)).mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.5025\n",
      "Recall: 0.5025\n",
      "Precision: 0.5025\n",
      "F-1 score: 0.5025\n",
      "Cross Validation Score: 0.5224\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Create the model.\n",
    "svm_model = svm.SVC()\n",
    "\n",
    "# Fit the training data\n",
    "svm_model.fit(x_train,y_train) # Fit the model using the training dataset\n",
    "\n",
    "# Pass the validation dataset and see the evaluation scores\n",
    "y_pred_valid = svm_model.predict(x_valid) # predict y_valid using x_valid\n",
    "evaluation(y_pred_valid, y_valid) # Run the evaluation function that gives evaluation scores\n",
    "print(\"Cross Validation Score:\", cross_val_score(svm_model, x_valid, y_valid, cv= KFold(10, shuffle=True, random_state = 22)).mean().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The multinomial Naive Bayes has higher score in terms of accuracy, recall, precision, F1 score and Cross validation score as compared to the SVM model. Although both the models do not have high metrics to actually implement it. Therefore, the model needs lot more data to learn from and this MIGHT be a case of underfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5mmYIfN8eYV"
   },
   "source": [
    "# **Question 3: House price prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsi2y4z88ngX"
   },
   "source": [
    "(40 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download here: https://github.com/unt-iialab/info5731_spring2021/blob/main/assignment/assignment4-question3-data.zip. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "XfvMKJjIXS5G"
   },
   "outputs": [],
   "source": [
    "# Import train and test data\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1452.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "      <td>1460.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>56.897260</td>\n",
       "      <td>70.049958</td>\n",
       "      <td>10516.828082</td>\n",
       "      <td>6.099315</td>\n",
       "      <td>5.575342</td>\n",
       "      <td>1971.267808</td>\n",
       "      <td>1984.865753</td>\n",
       "      <td>103.685262</td>\n",
       "      <td>443.639726</td>\n",
       "      <td>...</td>\n",
       "      <td>94.244521</td>\n",
       "      <td>46.660274</td>\n",
       "      <td>21.954110</td>\n",
       "      <td>3.409589</td>\n",
       "      <td>15.060959</td>\n",
       "      <td>2.758904</td>\n",
       "      <td>43.489041</td>\n",
       "      <td>6.321918</td>\n",
       "      <td>2007.815753</td>\n",
       "      <td>180921.195890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.610009</td>\n",
       "      <td>42.300571</td>\n",
       "      <td>24.284752</td>\n",
       "      <td>9981.264932</td>\n",
       "      <td>1.382997</td>\n",
       "      <td>1.112799</td>\n",
       "      <td>30.202904</td>\n",
       "      <td>20.645407</td>\n",
       "      <td>181.066207</td>\n",
       "      <td>456.098091</td>\n",
       "      <td>...</td>\n",
       "      <td>125.338794</td>\n",
       "      <td>66.256028</td>\n",
       "      <td>61.119149</td>\n",
       "      <td>29.317331</td>\n",
       "      <td>55.757415</td>\n",
       "      <td>40.177307</td>\n",
       "      <td>496.123024</td>\n",
       "      <td>2.703626</td>\n",
       "      <td>1.328095</td>\n",
       "      <td>79442.502883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1872.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>365.750000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>7553.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1954.000000</td>\n",
       "      <td>1967.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>129975.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>730.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>9478.500000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1994.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>383.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "      <td>163000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1095.250000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11601.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>712.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>214000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1460.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>313.000000</td>\n",
       "      <td>215245.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>5644.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>547.000000</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>508.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>15500.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id   MSSubClass  LotFrontage        LotArea  OverallQual  \\\n",
       "count  1460.000000  1460.000000  1201.000000    1460.000000  1460.000000   \n",
       "mean    730.500000    56.897260    70.049958   10516.828082     6.099315   \n",
       "std     421.610009    42.300571    24.284752    9981.264932     1.382997   \n",
       "min       1.000000    20.000000    21.000000    1300.000000     1.000000   \n",
       "25%     365.750000    20.000000    59.000000    7553.500000     5.000000   \n",
       "50%     730.500000    50.000000    69.000000    9478.500000     6.000000   \n",
       "75%    1095.250000    70.000000    80.000000   11601.500000     7.000000   \n",
       "max    1460.000000   190.000000   313.000000  215245.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\n",
       "count  1460.000000  1460.000000   1460.000000  1452.000000  1460.000000  ...   \n",
       "mean      5.575342  1971.267808   1984.865753   103.685262   443.639726  ...   \n",
       "std       1.112799    30.202904     20.645407   181.066207   456.098091  ...   \n",
       "min       1.000000  1872.000000   1950.000000     0.000000     0.000000  ...   \n",
       "25%       5.000000  1954.000000   1967.000000     0.000000     0.000000  ...   \n",
       "50%       5.000000  1973.000000   1994.000000     0.000000   383.500000  ...   \n",
       "75%       6.000000  2000.000000   2004.000000   166.000000   712.250000  ...   \n",
       "max       9.000000  2010.000000   2010.000000  1600.000000  5644.000000  ...   \n",
       "\n",
       "        WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  ScreenPorch  \\\n",
       "count  1460.000000  1460.000000    1460.000000  1460.000000  1460.000000   \n",
       "mean     94.244521    46.660274      21.954110     3.409589    15.060959   \n",
       "std     125.338794    66.256028      61.119149    29.317331    55.757415   \n",
       "min       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000       0.000000     0.000000     0.000000   \n",
       "50%       0.000000    25.000000       0.000000     0.000000     0.000000   \n",
       "75%     168.000000    68.000000       0.000000     0.000000     0.000000   \n",
       "max     857.000000   547.000000     552.000000   508.000000   480.000000   \n",
       "\n",
       "          PoolArea       MiscVal       MoSold       YrSold      SalePrice  \n",
       "count  1460.000000   1460.000000  1460.000000  1460.000000    1460.000000  \n",
       "mean      2.758904     43.489041     6.321918  2007.815753  180921.195890  \n",
       "std      40.177307    496.123024     2.703626     1.328095   79442.502883  \n",
       "min       0.000000      0.000000     1.000000  2006.000000   34900.000000  \n",
       "25%       0.000000      0.000000     5.000000  2007.000000  129975.000000  \n",
       "50%       0.000000      0.000000     6.000000  2008.000000  163000.000000  \n",
       "75%       0.000000      0.000000     8.000000  2009.000000  214000.000000  \n",
       "max     738.000000  15500.000000    12.000000  2010.000000  755000.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe() # General statistics for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1232.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1444.000000</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1458.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "      <td>1459.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2190.000000</td>\n",
       "      <td>57.378341</td>\n",
       "      <td>68.580357</td>\n",
       "      <td>9819.161069</td>\n",
       "      <td>6.078821</td>\n",
       "      <td>5.553804</td>\n",
       "      <td>1971.357779</td>\n",
       "      <td>1983.662783</td>\n",
       "      <td>100.709141</td>\n",
       "      <td>439.203704</td>\n",
       "      <td>...</td>\n",
       "      <td>472.768861</td>\n",
       "      <td>93.174777</td>\n",
       "      <td>48.313914</td>\n",
       "      <td>24.243317</td>\n",
       "      <td>1.794380</td>\n",
       "      <td>17.064428</td>\n",
       "      <td>1.744345</td>\n",
       "      <td>58.167923</td>\n",
       "      <td>6.104181</td>\n",
       "      <td>2007.769705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>421.321334</td>\n",
       "      <td>42.746880</td>\n",
       "      <td>22.376841</td>\n",
       "      <td>4955.517327</td>\n",
       "      <td>1.436812</td>\n",
       "      <td>1.113740</td>\n",
       "      <td>30.390071</td>\n",
       "      <td>21.130467</td>\n",
       "      <td>177.625900</td>\n",
       "      <td>455.268042</td>\n",
       "      <td>...</td>\n",
       "      <td>217.048611</td>\n",
       "      <td>127.744882</td>\n",
       "      <td>68.883364</td>\n",
       "      <td>67.227765</td>\n",
       "      <td>20.207842</td>\n",
       "      <td>56.609763</td>\n",
       "      <td>30.491646</td>\n",
       "      <td>630.806978</td>\n",
       "      <td>2.722432</td>\n",
       "      <td>1.301740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1461.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1470.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1879.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2006.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1825.500000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>7391.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1953.000000</td>\n",
       "      <td>1963.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>318.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2007.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2190.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>9399.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1973.000000</td>\n",
       "      <td>1992.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>350.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2008.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2554.500000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>11517.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2001.000000</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>753.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2919.000000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>56600.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>4010.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1488.000000</td>\n",
       "      <td>1424.000000</td>\n",
       "      <td>742.000000</td>\n",
       "      <td>1012.000000</td>\n",
       "      <td>360.000000</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>17000.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2010.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Id   MSSubClass  LotFrontage       LotArea  OverallQual  \\\n",
       "count  1459.000000  1459.000000  1232.000000   1459.000000  1459.000000   \n",
       "mean   2190.000000    57.378341    68.580357   9819.161069     6.078821   \n",
       "std     421.321334    42.746880    22.376841   4955.517327     1.436812   \n",
       "min    1461.000000    20.000000    21.000000   1470.000000     1.000000   \n",
       "25%    1825.500000    20.000000    58.000000   7391.000000     5.000000   \n",
       "50%    2190.000000    50.000000    67.000000   9399.000000     6.000000   \n",
       "75%    2554.500000    70.000000    80.000000  11517.500000     7.000000   \n",
       "max    2919.000000   190.000000   200.000000  56600.000000    10.000000   \n",
       "\n",
       "       OverallCond    YearBuilt  YearRemodAdd   MasVnrArea   BsmtFinSF1  ...  \\\n",
       "count  1459.000000  1459.000000   1459.000000  1444.000000  1458.000000  ...   \n",
       "mean      5.553804  1971.357779   1983.662783   100.709141   439.203704  ...   \n",
       "std       1.113740    30.390071     21.130467   177.625900   455.268042  ...   \n",
       "min       1.000000  1879.000000   1950.000000     0.000000     0.000000  ...   \n",
       "25%       5.000000  1953.000000   1963.000000     0.000000     0.000000  ...   \n",
       "50%       5.000000  1973.000000   1992.000000     0.000000   350.500000  ...   \n",
       "75%       6.000000  2001.000000   2004.000000   164.000000   753.500000  ...   \n",
       "max       9.000000  2010.000000   2010.000000  1290.000000  4010.000000  ...   \n",
       "\n",
       "        GarageArea   WoodDeckSF  OpenPorchSF  EnclosedPorch    3SsnPorch  \\\n",
       "count  1458.000000  1459.000000  1459.000000    1459.000000  1459.000000   \n",
       "mean    472.768861    93.174777    48.313914      24.243317     1.794380   \n",
       "std     217.048611   127.744882    68.883364      67.227765    20.207842   \n",
       "min       0.000000     0.000000     0.000000       0.000000     0.000000   \n",
       "25%     318.000000     0.000000     0.000000       0.000000     0.000000   \n",
       "50%     480.000000     0.000000    28.000000       0.000000     0.000000   \n",
       "75%     576.000000   168.000000    72.000000       0.000000     0.000000   \n",
       "max    1488.000000  1424.000000   742.000000    1012.000000   360.000000   \n",
       "\n",
       "       ScreenPorch     PoolArea       MiscVal       MoSold       YrSold  \n",
       "count  1459.000000  1459.000000   1459.000000  1459.000000  1459.000000  \n",
       "mean     17.064428     1.744345     58.167923     6.104181  2007.769705  \n",
       "std      56.609763    30.491646    630.806978     2.722432     1.301740  \n",
       "min       0.000000     0.000000      0.000000     1.000000  2006.000000  \n",
       "25%       0.000000     0.000000      0.000000     4.000000  2007.000000  \n",
       "50%       0.000000     0.000000      0.000000     6.000000  2008.000000  \n",
       "75%       0.000000     0.000000      0.000000     8.000000  2009.000000  \n",
       "max     576.000000   800.000000  17000.000000    12.000000  2010.000000  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.describe() # General statistics for testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries from scikit learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Drop null values from both test and train data.\n",
    "df_train = df_train.select_dtypes(include=['number']).interpolate().dropna()\n",
    "df_test = df_test.select_dtypes(include=['number']).interpolate().dropna()\n",
    "\n",
    "# SalePrice will be our target variable and the ID can be dropped since it doesnt add any value\n",
    "x_training = df_train.drop(['SalePrice','Id'], axis=1)\n",
    "y_training = np.log(df_train.SalePrice) # Use log so that the values are reduced for better accuracy and use exp later\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_training,y_training,random_state = 21, test_size=0.2)\n",
    "\n",
    "\n",
    "# Create the model and pass the training data\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(x_train,y_train)\n",
    "\n",
    "# Predict values using model.predict\n",
    "y_pred = regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression R squared\": 0.8634\n"
     ]
    }
   ],
   "source": [
    "# Check the score of the model\n",
    "print('Linear Regression R squared\": %.4f' % regressor.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i.e. 86.34% variability in X xan be explained with Y.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195747.28015953477\n"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE value\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lin_mse = mean_squared_error(np.exp(y_pred), y_test)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(lin_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the actual price of the house can differ with prediction value UP TO $195747.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted Prices</th>\n",
       "      <th>Actual Prices</th>\n",
       "      <th>Percentage Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>156677.762126</td>\n",
       "      <td>157000.0</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>229973.921577</td>\n",
       "      <td>205000.0</td>\n",
       "      <td>12.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>245358.948864</td>\n",
       "      <td>245350.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>218109.586021</td>\n",
       "      <td>210000.0</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>86909.329260</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>35.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>231419.797074</td>\n",
       "      <td>311500.0</td>\n",
       "      <td>25.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>311321.804719</td>\n",
       "      <td>367294.0</td>\n",
       "      <td>15.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>93234.996099</td>\n",
       "      <td>88000.0</td>\n",
       "      <td>5.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>109041.447359</td>\n",
       "      <td>118500.0</td>\n",
       "      <td>7.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>172431.414374</td>\n",
       "      <td>151000.0</td>\n",
       "      <td>14.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>292 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Predicted Prices  Actual Prices  Percentage Difference\n",
       "880      156677.762126       157000.0                   0.21\n",
       "605      229973.921577       205000.0                  12.18\n",
       "1166     245358.948864       245350.0                   0.00\n",
       "216      218109.586021       210000.0                   3.86\n",
       "970       86909.329260       135000.0                  35.62\n",
       "...                ...            ...                    ...\n",
       "218      231419.797074       311500.0                  25.71\n",
       "1228     311321.804719       367294.0                  15.24\n",
       "1007      93234.996099        88000.0                   5.95\n",
       "575      109041.447359       118500.0                   7.98\n",
       "599      172431.414374       151000.0                  14.19\n",
       "\n",
       "[292 rows x 3 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict = {\"Predicted Prices\":np.exp(y_pred),\"Actual Prices\":np.exp(y_test)}\n",
    "df_results = pd.DataFrame(results_dict)\n",
    "df_results[\"Percentage Difference\"] = round(abs((df_results[\"Predicted Prices\"] - df_results[\"Actual Prices\"]) / df_results[\"Actual Prices\"]) * 100,2)\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "INFO5731_Assignment_Four.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
