{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In-class-exercise-02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishalpuri07/VISHAL_INFO5731_FALL2021/blob/main/In_class_exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETrOSCzsfFMp"
      },
      "source": [
        "## Second In-class-exercise (9/15/2021, 40 points in total) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL9j_ECRfFMr"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B9ED4cnfFMr"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwbUvKii9mwL"
      },
      "source": [
        "While Mask Mandate seems to be more of a political debate meaning one political party agrees that wearing Mask reduces the cases of Covid-19 while the other party believes that it has no effect or even increases Covid-19 cases. So, a research question that begs to be asked is: \n",
        "\n",
        "\" Does Mask Mandate help supress the case of Covid-19 in schools?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blJpMNSy-t9a"
      },
      "source": [
        "To answer this question we basically will need following data:\n",
        "1. Be as granular as possible, practically possible to be as detailed as an area or case of Counties. Does a county have mask mandate in place or not? Different websites such as John Hopkins University's Covid data, Our World in Data's website, etc. can provide necessary dataset for this step. Create a dataset that contains information on name of counties and if the mask mandate is in place or not. Record the state as well.\n",
        "2. Once the data that contains name of county and record about the mask mandate. The next step is to collect the data to see if the Covid Cases on those counties are rising or not after the mask manate is on place or vice-versa. Be sure to state all assumptions at this point. These two are the main dataset required to answer the research question.\n",
        "3. Initiate randomness in process meaning what are the odds that Covid-Cases would rise or fall for a given county regardless of mask mandate. Lets say 35% of the time Covid Cases would tend to rise based on Histogram of the data irrespective of the Mask mandate. What percent of counties are seeing rising cases due to mask mandate in/ not in place.\n",
        "4. If there are more cases with certain confidence and p-interval than random chance due to mak mandate. The Covid Cases would be rising due to mask mandate and vice-versa.\n",
        "5. Since the data is not extremly large, a local computer would be sufficient to store the data and process it. \n",
        "\n",
        "It would be a good choice to collect as many data as possible for the purpose of this research. Since, there are only 3006 counties in United States and data for all counties might not be available, it is important to collect as much data as possible. Usually, these website mentioned provide data in CSV file especially for data containing about 3000 records. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlbW9JGfFMs"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 reviews of a movie from IMDB (https://www.imdb.com/) or 1000 reviews of a product from Amazon (https://www.amazon.com/).\n",
        "\n",
        "As for the IMDB movie review, the following informtion need to be collected (for example: https://www.imdb.com/title/tt6751668/reviews?ref_=tt_urv):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time\n",
        "\n",
        "\n",
        "As for the Amazon product review, the following information need to be collected (for example: https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_3?crid=2E3C55VKJX0K3&dchild=1&keywords=machine+learning+andrew+ng&qid=1631718619&sr=8-3):\n",
        "\n",
        "(1) User name\n",
        "\n",
        "(2) Star\n",
        "\n",
        "(3) Review title\n",
        "\n",
        "(4) Review text\n",
        "\n",
        "(5) Review posted time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sznIbIAqfFMs"
      },
      "source": [
        "# Import necessary Libraries such as urllib.request, bs4, pandas, etc.\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Create a soup using the link given in the question.\n",
        "link =  urllib.request.urlopen(r\"https://www.imdb.com/title/tt6751668/reviews?ref_=tt_urv\")\n",
        "soup = BeautifulSoup(link)\n",
        "\n",
        "# Now that the soup is created get different attributes required using the parsed html file.\n",
        "####### USERNAME #######\n",
        "Username = [] # create empty list so that the Username can be stored. \n",
        "for node in soup.findAll(class_ = \"display-name-link\"): # class - display-name-link contains Username of the reviewer.\n",
        "    Username.append((''.join(node.findAll(text=True)))) # get all the Username and create a list.\n",
        "    \n",
        "# TO clean the white spaces and new line character use strp()    \n",
        "for i in range(len(Username)): \n",
        "    Username[i] = str(Username[i]).strip()\n",
        "\n",
        "\n",
        "###### STAR #######    \n",
        "Star = [] # create empty list so that the Star/Rating can be stored. \n",
        "\n",
        "for node in soup.findAll(attrs={\"class\":\"rating-other-user-rating\"}): # Use attrs feature to extract ratings.\n",
        "    Star.append((''.join(node.findAll(text=True))))\n",
        "    \n",
        "# TO clean the white spaces and new line character use strp()    \n",
        "for i in range(len(Star)): \n",
        "    Star[i] = str(Star[i]).strip()\n",
        "\n",
        "Star.insert(10, \"Not Available\")\n",
        "Star.insert(15, \"Not Available\")   #Reviews for number 11th review is not availbale but is 10th element in DataFrame.\n",
        "\n",
        "\n",
        "####### REVIVEW TITLE #######\n",
        "Title = [] # create empty list so that the Title can be stored. \n",
        "for node in soup.findAll(class_ = \"title\"):  # \"title\" - class has the titles\n",
        "    Title.append((''.join(node.findAll(text=True))))\n",
        "    \n",
        "# TO clean the white spaces and new line character use strp()    \n",
        "for i in range(len(Title)): \n",
        "    Title[i] = str(Title[i]).strip()\n",
        "    \n",
        "    \n",
        "    \n",
        "####### REVIEW TEXT #######\n",
        "Text = [] # create empty list so that the Text can be stored. \n",
        "for node in soup.findAll(class_ = \"text show-more__control\"):\n",
        "    Text.append((''.join(node.findAll(text=True)))) # \"Text\" - class has the Texts for the reviews\n",
        "    \n",
        "# TO clean the white spaces and new line character use strp()    \n",
        "for i in range(len(Text)): \n",
        "    Text[i] = str(Text[i]).strip()\n",
        "    \n",
        "    \n",
        "    \n",
        "####### REVIEW POSTED TIME #######\n",
        "Time = [] # create empty list so that the Time can be stored. \n",
        "for node in soup.findAll(class_ = \"review-date\"):\n",
        "    Time.append((''.join(node.findAll(text=True)))) # \"Time\" - class has the Times at which the reviews were posted.\n",
        "    \n",
        "# TO clean the white spaces and new line character use strp()    \n",
        "for i in range(len(Time)): \n",
        "    Time[i] = str(Time[i]).strip()\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLFdxhMEJBsl"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K_vLeIkJCUZ"
      },
      "source": [
        "************* TO PRINT RESULTS ABOVE SIMPLY TYPE WHAT IS DESIRED***********\n",
        "\n",
        "\n",
        "\n",
        "            print(Username/ Star/ Title/ Text/ Time)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp-f3HNlfFMs"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/). \n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjZqAVyrfFMs"
      },
      "source": [
        "# Import all the necessary Libraries as before.\n",
        "\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Create Empty List so that the data is stored on this list later.\n",
        "\n",
        "Title = []\n",
        "Year = []\n",
        "Author = []\n",
        "Abstract = []\n",
        "Venue = []\n",
        "\n",
        "for page in range(10,490,10): # each page has 10 results. \n",
        "#THE WEBPAGE PROVIDES ONLY TOP 500 LIST BUT THE PROGRAM WORKS IF THE WEBSITE DID PROVIDE 1000 RESULTS.\n",
        "    link =  urllib.request.urlopen(f\"https://citeseerx.ist.psu.edu/search?q=Machine+Learning&t=doc&sort=rlv&start={page}\")\n",
        "# The URL Above is searching for term \"Machine Learning\" in the citeseerx website.\n",
        "    soup = BeautifulSoup(link) # Now the soup is ready which is a parsed HTML type document. \n",
        "    for i in soup.findAll(class_ = \"remove doc_details\"): # class for finding the Title\n",
        "        Title.append(i.get_text(strip=True)) # appends for all pages\n",
        "    for n in soup.findAll(class_ = \"pubvenue\"): # class for venue\n",
        "        Venue.append(i.get_text(strip=True))\n",
        "    for j in soup.findAll(class_ = \"pubyear\"): # Class for finding the Year\n",
        "        Year.append(j.get_text(strip = True)) # appends for all pages\n",
        "    for k in soup.findAll(class_ = \"authors\"): # Class for finding the Authors\n",
        "        Author.append(k.get_text(strip = True)) # appends for all pages\n",
        "    for l in soup.findAll(class_ = \"snippet\"): # class for finding the Abstract\n",
        "        Abstract.append(k.get_text(strip = True)) # appends for all pages\n",
        "    page = page + 1 # Move to next page. \n",
        "    \n",
        "    \n",
        "# TO clean the white spaces and new line character use strp()    \n",
        "for i in range(len(Author)): \n",
        "    Author[i] = str(Author[i].replace(\"by\",\"\").strip()) # removing leading \"by\" and whitespaces.\n",
        "\n",
        "\n",
        "for i in range(len(Year)): \n",
        "    Year[i] = str(Year[i].replace(\",\",\"\").strip())\n",
        "    \n",
        "    \n",
        "# TO clean the white spaces and new line character use strp()    \n",
        "for i in range(len(Abstract)): \n",
        "    Abstract[i] = str(Abstract[i].replace(\"...\",\"\").strip()) # removing leading by and whitespaces.\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuDWlrbwGMtk"
      },
      "source": [
        "************* TO PRINT RESULTS ABOVE SIMPLY TYPE WHAT IS DESIRED***********\n",
        "\n",
        "\n",
        "\n",
        "            print(Author/Year/Venue...)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OOSyjUzfFMs"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHL8aJ_z2dq6"
      },
      "source": [
        "# Import necessary libraries such as tweepy and Pandas.\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "# Once the Twitter Developer Account is created. The following information are required. \n",
        "API_Key = \"VekXOXop7n8jUgIq97nTeXtRE\"\n",
        "API_Key_Secret = \"55usREgICykrAZEoXEhwGGVZ63YIC9ZfuA5tTt6psPv0sy8ARD\"\n",
        "Access_Token = \"1438925460029808645-V7jg7AYvM132AeLDbsOdlYEAIXceRC\"\n",
        "Access_Token_Secret = \"fqc1lp9mMTy39pFKfJ7Nh0bsrunBz6WbWlt5I3ok3EyXt\"\n",
        "\n",
        "# The following steps provides an access to the Twitter using Twitter Developer API\n",
        "auth = tweepy.OAuthHandler(API_Key, API_Key_Secret)\n",
        "auth.set_access_token(Access_Token, Access_Token_Secret)\n",
        "api = tweepy.API(auth,wait_on_rate_limit=True)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "szMKCgtQ2o58",
        "outputId": "6bc6b77b-04b3-43bd-a7d2-b9409309f9da"
      },
      "source": [
        "Search_Term = 'Data Mining'\n",
        "max_tweets = 1000   #Number of tweets that you want to get.\n",
        " \n",
        "# Line of code required to set up the search and will be used below.\n",
        "tweets = tweepy.Cursor(api.search,q=Search_Term,lang='en').items(max_tweets)\n",
        " \n",
        "# Get Username, Posted Date and Text as requested in the question. \n",
        "info = [[tweet.user.name, tweet.created_at,  tweet.text] for tweet in tweets]\n",
        " \n",
        "# Creation a primary \n",
        "info_df = pd.DataFrame(info)\n",
        "\n",
        "# Display the necessary information.\n",
        "Results_df = pd.DataFrame({\"User Name\":info_df[0],\"Posted Time\":info_df[1], \"Text\": info_df[2]})\n",
        "Results_df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User Name</th>\n",
              "      <th>Posted Time</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>David L. Hutchinson</td>\n",
              "      <td>2021-09-20 02:49:50</td>\n",
              "      <td>@Aho2ToMan @tylermeredith Too true, #NDP data ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Marilyn Bayless</td>\n",
              "      <td>2021-09-20 02:45:44</td>\n",
              "      <td>RT @PoliticalEd3: @xxdr_zombiexx Data mining. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Graeme C.</td>\n",
              "      <td>2021-09-20 02:44:32</td>\n",
              "      <td>RT @samuelgbrooks: I've been curious throughou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Y‚óéK‚óé //</td>\n",
              "      <td>2021-09-20 02:39:32</td>\n",
              "      <td>RT @TheHeroShep: Been busy this weekend...\\n\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>travis</td>\n",
              "      <td>2021-09-20 02:31:30</td>\n",
              "      <td>RT @buffybvr: HAVE A GUITAR DENTIST POOPSIE JI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>Erica James</td>\n",
              "      <td>2021-09-17 23:47:45</td>\n",
              "      <td>RT @BeFixedTo: although the testing and data m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>Host Body Rachelle</td>\n",
              "      <td>2021-09-17 23:47:31</td>\n",
              "      <td>@SenatorJohnKane @santiagomayer_ This data min...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>üá¶üá™üá∂üá¶üá∏üá¶üá∫üá∏ Faisal Ben Sultan ŸÅŸäÿµŸÑ ÿ®ŸÜ ÿ≥ŸèŸÑŸíÿ∑ÿßŸÜ</td>\n",
              "      <td>2021-09-17 23:46:54</td>\n",
              "      <td>although the testing and data mining activity ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>rmarie699üá®üá¶üß°üïØüåà</td>\n",
              "      <td>2021-09-17 23:38:15</td>\n",
              "      <td>RT @TiMoudou: @MaxKingsleyEh @CTV Never ever e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>Marty Party</td>\n",
              "      <td>2021-09-17 23:24:23</td>\n",
              "      <td>@davidgokhshtein Check out @tagprotocol data-d...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows √ó 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      User Name  ...                                               Text\n",
              "0                           David L. Hutchinson  ...  @Aho2ToMan @tylermeredith Too true, #NDP data ...\n",
              "1                               Marilyn Bayless  ...  RT @PoliticalEd3: @xxdr_zombiexx Data mining. ...\n",
              "2                                     Graeme C.  ...  RT @samuelgbrooks: I've been curious throughou...\n",
              "3                                       Y‚óéK‚óé //  ...  RT @TheHeroShep: Been busy this weekend...\\n\\n...\n",
              "4                                        travis  ...  RT @buffybvr: HAVE A GUITAR DENTIST POOPSIE JI...\n",
              "..                                          ...  ...                                                ...\n",
              "995                                 Erica James  ...  RT @BeFixedTo: although the testing and data m...\n",
              "996                          Host Body Rachelle  ...  @SenatorJohnKane @santiagomayer_ This data min...\n",
              "997  üá¶üá™üá∂üá¶üá∏üá¶üá∫üá∏ Faisal Ben Sultan ŸÅŸäÿµŸÑ ÿ®ŸÜ ÿ≥ŸèŸÑŸíÿ∑ÿßŸÜ  ...  although the testing and data mining activity ...\n",
              "998                              rmarie699üá®üá¶üß°üïØüåà  ...  RT @TiMoudou: @MaxKingsleyEh @CTV Never ever e...\n",
              "999                                 Marty Party  ...  @davidgokhshtein Check out @tagprotocol data-d...\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFZDx_JiGgim"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}